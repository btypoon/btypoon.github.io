<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>My note</title>
  </head>

  <body>
    <main>
      <h1>
        Standard Operating Procedure: Management and Troubleshooting Guide for
        the EdUHK Physics Laboratory's Server
      </h1>
      <p>
        This document serves as a manual for the administration and maintenance
        of the EdUHK Physics Laboratory's Server. It provides instructions for
        accessing and operating the server, outlines current unresolved issues,
        and documents historical problems with its step-by-step solutions. It is
        designed to ensure operational consistency, facilitate troubleshooting,
        and support continuity for future system administrators.
      </p>
      <hr />
      <h1>Table of content</h1>
      <figure>
        <ol class="toc">
          <li>
            <a href="#cluster_architecture"></a>
            <ol class="sub-toc">
              <li><a href="#cluster1"></a></li>
              <li><a href="#cluster2"></a></li>
            </ol>
          </li>
          <li>
            <a href="#booting_sequence"></a>
            <ol class="sub-toc">
              <li><a href="#boot1"></a></li>
              <li><a href="#boot2"></a></li>
            </ol>
          </li>
          <li><a href="#connect_to_the_server"></a></li>
          <li><a href="#new_user"></a></li>
          <li><a href="#list_format"></a></li>
          <li><a href="#etc_groups"></a></li>
          <li><a href="#problem1"></a></li>
          <li>
            <a href="#problem2"></a>
            <ol class="sub-toc">
              <li><a href="#problem2.1"></a></li>
              <li><a href="#problem2.2"></a></li>
              <li><a href="#problem2.3"></a></li>
            </ol>
          </li>
          <li><a href="#problem3"></a></li>
          <li>
            <a href="#problem4"></a>
            <ol class="sub-toc">
              <li>
                <a href="#problem4.1"></a>
                <ol class="sub-toc">
                  <li><a href="#ubuntu_apt-upgrade"></a></li>
                  <li><a href="#ubuntu_do-release-upgrade"></a></li>
                  <li><a href="#problem4.1.1"></a></li>
                  <li><a href="#problem4.1.2"></a></li>
                  <li><a href="#problem4.1.3"></a></li>
                  <li><a href="#problem4.1.4"></a></li>
                </ol>
              </li>
              <li>
                <a href="#problem4.2"></a>
                <ol class="sub-toc">
                  <li><a href="#problem4.2.1"></a></li>
                  <li><a href="#problem4.2.2"></a></li>
                </ol>
              </li>
              <li><a href="#problem4.3"></a></li>
            </ol>
          </li>
          <li>
            <a href="#problem5"></a>
            <ol class="sub-toc">
              <li><a href="#12.1"></a></li>
              <li><a href="#12.2"></a></li>
              <li><a href="#12.3"></a></li>
            </ol>
          </li>
          <li><a href="#problem6"></a></li>
          <li><a href="#problem7"></a></li>
          <li>
            <a href="#direct_reinstall"></a>
            <ol class="sub-toc">
              <li><a href="#backupData"></a></li>
              <li><a href="#ssh_setup"></a></li>
              <li><a href="#desktop_setup"></a></li>
              <li><a href="#hostname_setup"></a></li>
            </ol>
          </li>
          <li>
            <a href="#NFS"></a>
            <ol class="sub-toc">
              <li><a href="#NFS_server"></a></li>
              <li><a href="#NFS_client"></a></li>
              <li><a href="#NFS_investigation"></a></li>
            </ol>
          </li>
        </ol>
      </figure>
      <hr />
      <section>
        <h2 id="cluster_architecture">Cluster architecture</h2>
        <div class="datetime"><time>10-Auguest-2023</time></div>
        <div class="content">
          <p>
            There are the seven computers in our server cluster. Their hostname
            are listed below:
          </p>
          <ol>
            <li>bill01</li>
            <li>master</li>
            <li>slave1</li>
            <li>slave2</li>
            <li>slave3</li>
            <li>slave4</li>
            <li>slave5</li>
          </ol>
          <p>Grid Engine was used for job scheduling in our server.</p>
          <section>
            <h3 id="cluster1">Grid Engines</h3>
            <div class="datetime"><time>12-Auguest-2025</time></div>
            <div class="content">
              <p>
                My research found that the original Sun Grid Engine was
                developed by Oracle and discontinued in 2010. Their official
                website was closed in 2010. Many alternative fork of the grid
                engine exists nowadays, such as Open Cluster Scheduler or Some
                Grid Engine. There is hardly any installation guide left on the
                internet regarding the original Sun grid engine. I managed to
                salvaged the last two version of user guide (<a
                  href="image/cluster/Sun N1 Grid Engine 6.1 User's Guide.pdf"
                  >v6.1</a
                >,
                <a
                  href="image/cluster/Sun_Grid_Engine_6.2_install_and_config.pdf"
                  >v6.2</a
                >) published by Sun Microsystem. And I suggest we should migrate
                to newer fork of grid engines.
              </p>
              <p>
                In 2023, we would install the Grid Engine, using:
                <span class="command">
                  sudo apt-get install gridengine-client gridengine-execute
                </span>
              </p>
              <p>
                The package name
                <a href="https://packages.ubuntu.com/source/plucky/gridengine"
                  >gridengine 8.1.9</a
                >
                suggested it was actually "Son of Grid Engine" developed by
                Advanced Research Computing center at the University of
                Liverpool that we were using. It was also a fork of the original
                Sun Grid Engine by Oracle. Its official webpage (<a
                  href="arc.liv.ac.uk"
                  >arc.liv.ac.uk</a
                >) was now shutdown too and inaccessable. Looking at the last
                available online source of the packages seems to suggest that it
                was last updated and discontinued at <a
                  href="https://stab.st-andrews.ac.uk/wiki/index.php/Son_of_Gridengine"
                  >March 2016</a
                >.
              </p>
              <p>
                <a href="https://github.com/daimh/sge">Some Grid Engine</a>,
                developed by University of Michigan, seems to be a successor and
                a fork of Son of Grid Engine. Making it a fork of fork of the
                original Sun Grid Engine. It is free and currently still
                being maintained.
              </p>
              <p>
                There is also another free open source Grid engine
                <a href="https://github.com/hpc-gridware/clusterscheduler"
                  >Open Cluster Scheduler</a
                >, based on the Open Cluster Scheduler (OCS), which itself
                originates from the Univa Open Core Grid Engine, which in turn
                was derived from the open-source Sun Grid Engine. It seems to be
                developed by a private company and also under active support
                and development.
              </p>
              <p>
                Since our OS reinstallation of our server in 2025, we have to
                reinstall the Grid Engine from scratch. As the official website
                for both Orcale Sun Grid Engine (2010) and University of
                Liverpool's Son of Grid Engine (2016) are both shutdown and
                inaccessible, there are not any official guide left regarding
                the installation of their Grid engine. I suggest installing Some
                Grid Engine instead of going back to using Son of Grid Engine.
              </p>
              <figure>
                <img src="image/cluster/timeline.webp" alt="" />
                <figcaption>
                  Timeline of the Grid Engine development.
                </figcaption>
              </figure>
            </div>
          </section>
          <section>
            <h3 id="cluster2">Architecture</h3>
            <div class="datetime"><time>12-Auguest-2025</time></div>
            <div class="content">
              <p>
                Originally, our server setup consist of a client node, a master
                node and five execution nodes. User connect to the client node
                via ssh connection. Client node has grid engine installed.
                Therefore, it can submit job to master node, which distribute
                the job to the execution nodes.
              </p>
              <figure>
                <img
                  width="700"
                  src="image/cluster/architecture_old.webp"
                  alt=""
                />
                <figcaption>Original server setup before 2025</figcaption>
              </figure>
              <p>
                However I think, the the client node was a redundancy. The real
                server became the other six computers. When there is nobody
                connecting to the server, the client node is just idling and
                master node is just in standby mode. To be honest, any client
                computer with grid engine installed and connectivity to master
                nodes can directly submit job to master without the
                need to connect to client node first. Therefore, the
                computational power of both client node and master node were
                wasted.
              </p>
              <p>
                I suggested to merge both the job of a client node and master
                node to one computer, freeing another computer. The extra
                computer can become another execution node. In this setup, we
                have six execution nodes instead of five. User without grid
                engine installed on their computer connect directly to master
                node firstly, then submit their job secondly.
              </p>
              <figure>
                <img
                  width="700"
                  src="image/cluster/architecture_new.webp"
                  alt=""
                />
                <figcaption>My suggested server setup in 2025</figcaption>
              </figure>
            </div>
          </section>
        </div>
      </section>
      <section>
        <h2 id="booting_sequence">Booting Sequence</h2>
        <div class="content">
          <section>
            <h3 id="boot1">Bill01</h3>
            <div class="datetime"><time>21-Auguest-2025</time></div>
            <div class="content">
              <p>
                The BIOS of <em>Bill01</em> was password protected, and nobody
                know the password. Moreover, the boot priority was set to
                prioritize the USB drive over the harddrive. Therefore, upon
                every reboot, the system would display an error indicating a
                missing operating system, since no USB with an OS was inserted
                at that time. There is no way to change the boot priority
                without the password.
              </p>
              <p>
                Potential solutions included removing and reinserting the CMOS
                battery on the motherboard. Or removing the password reset
                jumper on the motherboard and rebooting the computer. However,
                both methods require taking out the computer and disassembling
                it, which we prefer to avoid due to the associated
                inconvenience.
              </p>
              <p>
                A temporary workaround is to access the BIOS setting on every
                startup and manually select
                <span class="inline">Ubuntu</span> as the boot drive.
              </p>
            </div>
          </section>
          <section>
            <h3 id="boot2">Slave4</h3>
            <div class="datetime"><time>10-Auguest-2023</time></div>
            <div class="content">
              <p>
                <em>Slave4</em> require special steps in order to be booted
                successfully. When you face "The Red Screen of Death", follow
                these steps:
              </p>
              <ol>
                <li>Press F11 to go into BIOS while booting</li>
                <li>Change the booting mode to UEFI</li>
                <li>Save and reboot</li>
              </ol>
            </div>
          </section>
        </div>
      </section>
      <section>
        <h2 id="connect_to_the_server">Connect to the server</h2>
        <div class="datetime"><time>10-Auguest-2023</time></div>
        <div class="content">
          <p>
            To connect to the server, you must first connected to the university
            intranet. You may use either:
          </p>
          <ul>
            <li>Wifi <span class="inline">EdUHK</span></li>
            <li>Wifi <span class="inline">IoT</span></li>
            <li>Wired university computer</li>
            <li>EdUHK VPN</li>
          </ul>
          <p>
            Use any ssh software of your choice such as
            <i>PuTTY</i> or window terminal. You can connect to either of the
            following IP addresses via <span class="inline">ssh</span> command:
          </p>
          <ul>
            <li><span class="inline">ssh phys.eduhk.hk</span></li>
            <li><span class="inline">ssh phys.ied.edu.hk</span></li>
            <li><span class="inline">ssh 175.159.131.248</span></li>
          </ul>
          <p>
            Enter your username and password when prompted. A quick way to
            connect the server with a specific username is to use
            <span class="inline">ssh username@IP</span>
          </p>
          <p>
            When connected successfully, you will be greeted by the following
            messages.
          </p>
          <pre><code class="language-bash">
"login as: username
username@phys.eduhk.hk's password:
Welcome to Ubuntu 16.04.2 LTS (GNU/Linux 4.8.0-36-generic x86_64)

* Documentation:  https://help.ubuntu.com
* Management:     https://landscape.canonical.com
* Support:        https://ubuntu.com/advantage

Last login: Week Month Day Time Year from IP_address
username@bill01:~$"
            </code></pre>
          <p>
            The entrance point of the server is <i>bill01</i>. To connect to any
            other six computers, type the command
            <span class="inline">ssh hostname</span> with the hostname of the
            other computer.
          </p>
        </div>
      </section>
      <section>
        <h2 id="new_user">Adding new user</h2>
        <div class="datetime"><time>11-Auguest-2025</time></div>
        <div class="content">
          <p>
            To Add a new user, run
            <span class="command">
              sudo
              <a
                href="https://manpages.ubuntu.com/manpages/noble/man8/adduser.8.html"
              >
                adduser
              </a>
              username
            </span>
            with administrative privileges.
          </p>
          <p>
            If you ever encountered a problem that user cannot write to their
            own home directory, their home directory might not be initialized
            successfully and is owned by root. Check permission with command:
            <span class="command">ls -l /home</span>
          </p>
          <p>
            To fix that, delete the user completely and recreate the user
            account.
            <span class="command">
              sudo
              <a
                href="https://manpages.ubuntu.com/manpages/noble/en/man8/deluser.8.html"
              >
                deluser
              </a>
              --remove-all-files username
            </span>
          </p>
          <p>
            To list all the human user, run the command:
            <span class="command">awk -F: '$3 >= 1000' /etc/passwd | sort</span>
          </p>
          <p>
            Good record of the user information is recommended to keep track of
            the ownership of the username. To modify user information, please
            run the following command:
            <span class="command"
              >sudo
              <a
                href="https://manpages.ubuntu.com/manpages/noble/en/man1/chfn.1.html"
              >
                chfn
              </a>
              username
            </span>
          </p>
          <p>It will prompt user to input their</p>
          <ul>
            <li>Full name</li>
            <li>Room number</li>
            <li>Work phone</li>
            <li>Home phone</li>
            <li>Other</li>
          </ul>
          <p>
            As of 30/6/2023, the file in
            <span class="inline">/etc/passwd</span> read:
          </p>
          <pre><b>/etc/passwd</b><code class="language-bash">
#########################################################
root:x:0:0:root:/root:/bin/bash
daemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologin
bin:x:2:2:bin:/bin:/usr/sbin/nologin
sys:x:3:3:sys:/dev:/usr/sbin/nologin
sync:x:4:65534:sync:/bin:/bin/sync
games:x:5:60:games:/usr/games:/usr/sbin/nologin
man:x:6:12:man:/var/cache/man:/usr/sbin/nologin
lp:x:7:7:lp:/var/spool/lpd:/usr/sbin/nologin
mail:x:8:8:mail:/var/mail:/usr/sbin/nologin
news:x:9:9:news:/var/spool/news:/usr/sbin/nologin
uucp:x:10:10:uucp:/var/spool/uucp:/usr/sbin/nologin
proxy:x:13:13:proxy:/bin:/usr/sbin/nologin
www-data:x:33:33:www-data:/var/www:/usr/sbin/nologin
backup:x:34:34:backup:/var/backups:/usr/sbin/nologin
list:x:38:38:Mailing List Manager:/var/list:/usr/sbin/nologin
irc:x:39:39:ircd:/var/run/ircd:/usr/sbin/nologin
gnats:x:41:41:Gnats Bug-Reporting System (admin):/var/lib/gnats:/usr/sbin/nologin
nobody:x:65534:65534:nobody:/nonexistent:/usr/sbin/nologin
systemd-timesync:x:100:102:systemd Time Synchronization,,,:/run/systemd:/bin/false
systemd-network:x:101:103:systemd Network Management,,,:/run/systemd/netif:/bin/false
systemd-resolve:x:102:104:systemd Resolver,,,:/run/systemd/resolve:/bin/false
systemd-bus-proxy:x:103:105:systemd Bus Proxy,,,:/run/systemd:/bin/false
syslog:x:104:108::/home/syslog:/bin/false
_apt:x:105:65534::/nonexistent:/bin/false
messagebus:x:106:110::/var/run/dbus:/bin/false
uuidd:x:107:111::/run/uuidd:/bin/false
lightdm:x:108:114:Light Display Manager:/var/lib/lightdm:/bin/false
whoopsie:x:109:116::/nonexistent:/bin/false
avahi-autoipd:x:110:119:Avahi autoip daemon,,,:/var/lib/avahi-autoipd:/bin/false
avahi:x:111:120:Avahi mDNS daemon,,,:/var/run/avahi-daemon:/bin/false
dnsmasq:x:112:65534:dnsmasq,,,:/var/lib/misc:/bin/false
colord:x:113:123:colord colour management daemon,,,:/var/lib/colord:/bin/false
speech-dispatcher:x:114:29:Speech Dispatcher,,,:/var/run/speech-dispatcher:/bin/false
hplip:x:115:7:HPLIP system user,,,:/var/run/hplip:/bin/false
kernoops:x:116:65534:Kernel Oops Tracking Daemon,,,:/:/bin/false
pulse:x:117:124:PulseAudio daemon,,,:/var/run/pulse:/bin/false
rtkit:x:118:126:RealtimeKit,,,:/proc:/bin/false
saned:x:119:127::/var/lib/saned:/bin/false
usbmux:x:120:46:usbmux daemon,,,:/var/lib/usbmux:/bin/false
bill:x:1000:1000:bill01,,,:/home/bill:/bin/bash
postfix:x:121:129::/var/spool/postfix:/bin/false
sgeadmin:x:122:131::/var/lib/gridengine:/bin/false
sshd:x:123:65534::/var/run/sshd:/usr/sbin/nologin
hadoopuser:x:1001:1001:,,,:/home/hadoopuser:/bin/bash
testuser:x:1002:1002:,,,:/home/testuser:/bin/bash
jacky:x:1003:1003:,,,:/home/jacky:/bin/bash
takshing:x:1004:1004:,,,:/home/takshing:/bin/bash
cheong:x:1005:1005:,,,:/home/cheong:/bin/bash
lim:x:1006:1006:,,,:/home/lim:/bin/bash
curtis:x:1007:1007:,,,:/home/curtis:/bin/bash
statd:x:1008:1008:,,,:/home/statd:/bin/bash
bil:x:1009:1009:,,,:/home/bil:/bin/bash
kcchoi:x:1010:1010:,,,:/home/kcchoi:/bin/bash
roy:x:1011:1011:,,,:/home/roy:/bin/bash
leo:x:1012:1012:,,,:/home/leo:/bin/bash
intern:x:1013:1013:,,,:/home/intern:/bin/bash
william:x:1014:1014:,,,:/home/william:/bin/bash
brian:x:1015:1015:,,,:/home/brian:/bin/bash
paul:x:1016:1016:,,,:/home/paul:/bin/bash
btypoon:x:1017:1017:,,,:/home/btypoon:/bin/bash
#########################################################
            </code></pre>
        </div>
      </section>
      <section>
        <h2 id="list_format">Long format listing ls -l</h2>
        <div class="datetime"><time>10-Auguest-2023</time></div>
        <div class="content">
          <p>
            Using
            <span class="inline"
              ><a
                href="https://manpages.ubuntu.com/manpages/noble/en/man1/ls.1posix.html"
                >ls</a
              >
              -l</span
            >
            to list long format of files. The columns coorespond to:
          </p>
          <ol>
            <li>The file type.</li>
            <li>The file permissions.</li>
            <li>Number of hard links to the file.</li>
            <li>File owner.</li>
            <li>File group.</li>
            <li>File size.</li>
            <li>Date and Time.</li>
            <li>File name.</li>
          </ol>
        </div>
      </section>
      <section>
        <h2 id="etc_groups">/etc/groups</h2>
        <div class="datetime"><time>10-Auguest-2023</time></div>
        <div class="content">
          <p>
            To view all groups, visit
            <span class="inline">/etc/groups</span>. To change the uid or gid of
            a user, use:
            <span class="command"
              >sudo
              <a
                href="https://manpages.ubuntu.com/manpages/noble/en/man8/usermod.8.html"
              >
                usermod
              </a>
              -u uid -g gid username</span
            >
            To change the gid or group name of a group, use:
            <span class="command"
              >sudo
              <a
                href="https://manpages.ubuntu.com/manpages/noble/en/man8/groupmod.8.html"
              >
                groupmod
              </a>
              -g gid -n new_groupName old_groupName</span
            >
          </p>
        </div>
      </section>
      <section>
        <h2 id="problem1">
          Problem 1 (obsolete): Looking for a faster way to add new user on all
          computers
        </h2>
        <div class="datetime"><time>19-Auguest-2023</time></div>
        <div class="content">
          <p>
            In 2023, we needed to create the same user account on all machines
            by running the command
            <span class="inline">sudo adduser</span> repeatingly. We were
            looking for solution to add the same user on all computer
            simultaneously and efficiently.
          </p>
          <p>
            Maybe, there is a way to synchronize the user data on all computers.
            Or one might write a script to automatically create a new user with
            the necessary credential information on all computers in one go
          </p>
          <p>
            In 2025, we now recommanded to only create new user on
            <span class="inline">Bill01</span>. So, we can restrict user from
            accessing any of the slaves computers. As of 2025, after the
            reinstallation of Ubuntu OS v24.04 onto all slaves computers, only
            the admin user <i>Bill</i> exists on them.
          </p>
          <p>Therefore, this problem is now obsolete.</p>
        </div>
      </section>
      <section>
        <h2 id="problem2">
          Problem 2 (obsolete): Unable to create or modify files in all Slave
          computers for new user
        </h2>
        <div class="datetime"><time>5-July-2023</time></div>
        <div class="content">
          <p>
            Our new user Thomas Ho cannot run program or make any changes in all
            slaves computers but bill01. Older account does not have this
            problem
          </p>
          <section>
            <h3 id="problem2.1">Located Cause</h3>
            <div class="content">
              <p>
                The issue was that, although the user <em>ThomasHo</em> has the
                same username on both <em>Bill01</em> and the
                <em>Slave</em> computers, their UID and GID do not match. The
                directory <span class="inline">/home/ThomasHo</span> on the
                <em>Slave</em> computers are owned and grouped based on UID and
                GID, rather than the username. As a result, when user
                <em>ThomasHo</em> from <em>Bill01</em> attempts to access
                <span class="inline">/home/ThomasHo</span> on the
                <em>Slave</em> computers, the system does not recognize him as
                the owner of the directory.
              </p>
            </div>
          </section>
          <section>
            <h3 id="problem2.2">Suggested solution</h3>
            <div class="content">
              <p>
                The problem was resolved by granting permissions to
                <em>others</em> for his home directory. However, this change
                also allowed any other user, in addition to
                <em>ThomasHo</em> from <em>Bill01</em>, to read, write, and
                execute files within his home directory.
              </p>
            </div>
          </section>
          <section>
            <h3 id="problem2.3">
              New problem (obsolete): Different group id for same user across
              all computers
            </h3>
            <div class="content">
              <figure>
                <img width="300" src="image\cluster\problem1.webp" alt="" />
                <figcaption>
                  Above shows the file
                  <span class="inline">/etc/groups</span> in computer bill01
                  (left), slave1(middle) and slave2(right). Their fields are
                  represented as group_name:password:GID:Group List.
                </figcaption>
              </figure>
              <p>
                As I digged deeper into the files, I realise new users have
                mismatched UID and GID.
              </p>
              <p>
                In Linux, each user is assigned a UID and GID when created. File
                permissions are determined based on these IDs. The system
                recognizes the owner and group of a file by matching UIDs and
                GIDs.
              </p>
              <p>
                An example of this issue can be seen with user <em>btypoon</em>:
                on the <em>Bill01</em> system, <em>btypoon</em> has GID 1017,
                but on the <em>Slave</em> computer, <em>btypoon</em> has GID
                1016. Further mismatches can be seen starting from user
                <em>curtis</em> (GID 1007) and onwards.
              </p>
              <p>
                To fix that, we will need to synchronize the UID and GID across
                all seven computers. However, this process is time-consuming.
                Therefore, I recommended simply making the user's home directory
                writable for all users as a practical solution.
              </p>
            </div>
          </section>
          <p>
            As of 2025, we reinstalled the OS of all computers and erased all
            user data. Only the admin user <em>Bill</em> will be existing on all
            <em>Slave</em> computers.
          </p>
        </div>
      </section>

      <h2 id="problem3">
        Problem 3: Unable to run python program on the server
      </h2>
      <section>
        <div class="datetime"><time>9-Dec-2023</time></div>
        <div class="content">
          <p>
            One of the staff, Nesat, reported that she was unable to run her
            python program on our Physics Lab Server. Specifically, she wanted
            to install the python packages "Tensorflow", which her program rely
            on, but unsuccessful. This problem remain unresolved. Further
            investigation is required.
          </p>
          <section>
            <h3>Suggested solution</h3>
            <div class="datetime"><time>21-Augest-2025</time></div>
            <div class="content">
              <p>
                As of 2025, we realised that Ubuntu relies heavily on Python.
                Consequently, we cannot let user to freely install or modify
                Python modules as it might lead to conflicts with the system's
                Python environment and potentially cause core functionality
                break down and system corruption.
              </p>
              <p>
                We look for ways to set up virtual environments for each user
                and allow use to install modules within their local python
                envrionments.
              </p>
              <p>
                Currently, a Python tool called
                <span class="inline">PyInstaller</span> might be the solution to
                our problem.
              </p>
            </div>
          </section>
        </div>
      </section>

      <section>
        <h2 id="problem4">
          Problem 4: Security reported three critical vulnerabilities
        </h2>
        <div class="datetime"><time>11-Jul-2025</time></div>
        <div class="content">
          <p>
            According to the Tenable Security Report provided by the OCIO, three
            vulnerabilities had been identified on the Physics Laboratory's
            Server:
          </p>
          <ol>
            <li>Canonical Ubuntu Linux SEoL (16.04.x) - Critical Severity</li>
            <li>
              SSH Terrapin Prefix Truncation Weakness (CVE-2023-48795) - High
              Severity
            </li>
            <li>
              ICMP Timestamp Request Remote Date Disclosure - Low Severity
            </li>
          </ol>
          <section>
            <h3 id="problem4.1">
              First vulnerability - Canonical Ubuntu Linux SEoL (16.04.x) -
              Critical Severity
            </h3>
            <div class="content">
              <p>
                Regarding the first critical vulnerability, our server was
                running Ubuntu version 16.04, which reached its end-of-life in
                2021 and was no longer supported by the Official. Upgrading the
                operating system to a supported and up-to-date version will
                effectively resolve this issue.
              </p>
              <p>
                To reserve all the user data and system settings, we desired to
                not do a comeplete erase and reinstallation of the OS. So, we
                decided to upgrade the OS using the system upgrader.
              </p>
              <p>
                However, Ubuntu does not support a direct upgrade from version
                16.04 to the latest release, 24.04. Instead, the upgrade must be
                performed incrementally from 16.04 → 18.04 → 20.04 → 22.04 →
                24.04.
              </p>
              <p>
                For each upgrade step, the following commands should be
                executed:
              </p>
              <pre><code class="language-bash">
sudo apt update
sudo apt dist-upgrade
sudo reboot
sudo do-release-upgrade
              </code></pre>
              <p>
                The upgrade process must be repeated for each of the seven
                computers in our server cluster, marking a total of 28 OS
                upgrades. This made the overall effort very time-consuming.
              </p>
              <p>
                To make the process effective, the following two shell script
                were written and to be executed in sequence on each of the OS
                upgrades.
              </p>
              <pre><b id="ubuntu_apt-upgrade">ubuntu_apt-upgrade.sh</b><code class="language-bash">
#!/bin/bash

# Exit if any command fails
set -e
# Ensure the script is run as root
if [ "$EUID" -ne 0 ]; then
  echo "❌ Please run as root or with sudo"
  exit 1
fi

# Ubuntu Upgrade Automation Script: 16.04 → 24.04 LTS
function upgrade_stage() {
    echo "======================================"
    echo "Preparing system for release upgrade from Ubuntu $1"
    echo "======================================"
    read -p "This script will update your system and drivers. Press Enter to continue, or Ctrl+C to cancel."


    echo "======================================"
    echo "Step 1: Updating package list..."
    echo "======================================"
    apt update

    echo "======================================"
    echo "Step 2: Performing dist-upgrade..."
    echo "======================================"
    apt dist-upgrade -y


    echo "======================================"
    echo "Step 3: Removing unused packages."
    echo "======================================"
    apt autoremove -y


    echo "======================================"
    echo "Step 4: Checking for recommended drivers..."
    echo "======================================"
    ubuntu-drivers devices


    echo "======================================"
    echo "Step 5: Ready to install recommended drivers."
    echo "======================================"
    read -p "Press Enter to proceed, or Ctrl+C to cancel."
    ubuntu-drivers autoinstall

    echo "======================================"
    echo "Step 6: Rebooting system..."
    echo "======================================"
    read -p "Press Enter to reboot now, or Ctrl+C to cancel."
    reboot
}



# Detect current version
#using /etc/os-release instead of lsb_release for newer OS
if [[ -f /etc/os-release ]]; then
    current_version=$(grep "^VERSION_ID=" /etc/os-release | cut -d '"' -f 2)
else
    echo "Cannot detect OS version. /etc/os-release not found."
    exit 1
fi

upgrade_stage "$current_version"
              </code></pre>
              <pre><b id="ubuntu_do-release-upgrade">ubuntu_do-release-upgrade.sh</b><code class="language-bash">
#!/bin/bash

# Exit if any command fails
set -e
# Ensure the script is run as root
if [ "$EUID" -ne 0 ]; then
  echo "❌ Please run as root or with sudo"
  exit 1
fi

# Ubuntu Upgrade Automation Script: 16.04 → 24.04 LTS
function release_upgrade() {
    echo "======================================"
    echo "OS upgrading from Ubuntu $1"
    echo "Performing release upgrade..."
    echo "======================================"

    if do-release-upgrade -f DistUpgradeViewNonInteractive; then
        echo "Standard release upgrade succeeded."
    else
        echo "Standard upgrade failed. Trying development release upgrade..."
        do-release-upgrade -d -f DistUpgradeViewNonInteractive
    fi

    echo "Release upgrade completed. Please reboot and re-run the script to continue."
    read -p "Press Enter to reboot now."
    reboot
}


# Detect current version 
#using /etc/os-release instead of lsb_release for newer OS
if [[ -f /etc/os-release ]]; then
    current_version=$(grep "^VERSION_ID=" /etc/os-release | cut -d '"' -f 2)
else
    echo "Cannot detect OS version. /etc/os-release not found."
    exit 1
fi


release_upgrade "$current_version"
              </code></pre>
              <p>
                To check the current OS version, one may use the following
                commands for version before 18.04:
                <span class="command">lsb_release -a</span>
                and after 20.04:
                <span class="command">cat /etc/os/release</span>
              </p>
              <section>
                <h4 id="problem4.1.1">
                  Problem 4.1.1 - Outdated Graphic Driver
                </h4>
                <div class="content">
                  <p>
                    During the upgrade process of computers <i>Bill01</i> and
                    <i>master</i>, the display output was lost due to outdated
                    graphics drivers that were incompatible with the newer
                    version of Ubuntu.
                  </p>
                  <p>To resolve this issue, the following steps were taken:</p>
                  <ol>
                    <li>
                      <b>Identifying the Graphics Card</b>
                      <p>
                        The following command was ran to determine the video
                        card installed on the system:
                        <span class="command">sudo lshw -c video</span>
                      </p>
                    </li>
                    <li>
                      <b>Installing Driver Management Module (if missing)</b>
                      <p>
                        For machine that do not have ubuntu-drivers utility
                        installed, we ran:
                        <span class="command"
                          >sudo apt install ubuntu-drivers-common</span
                        >
                      </p>
                    </li>
                    <li>
                      <b>Check for Available Drivers</b>
                      <p>
                        Using the command below, we listed any new and available
                        drivers:
                        <span class="command">ubuntu-drivers devices</span>
                      </p>
                    </li>
                    <li>
                      <b>Installing new Drivers</b>
                      <p>
                        To install the new graphics drivers, we ran:
                        <span class="command"
                          >sudo ubuntu-drivers autoinstall</span
                        >
                      </p>
                    </li>
                    <li>
                      <b>Reboot the System</b>
                      <p>
                        After installation, we rebooted the system to apply the
                        changes:<span class="command">sudo reboot</span>
                      </p>
                    </li>
                  </ol>
                  <p>
                    Afterwards, the display functionality was restored and the
                    upgrade process can be continued.
                  </p>
                </div>
              </section>
              <section>
                <h4 id="problem4.1.2">Problem 4.1.2 - "DPKG" packages error</h4>
                <div class="content">
                  <p>
                    During the upgrade process of computers <i>Slave1-5</i>, all
                    encountered the error:
                    <span class="inline"
                      >"Sub-process /usr/bin/dpkg returned an error code
                      (1)"</span
                    >, preventing them from being upgradable.
                  </p>
                  <p>
                    Extensive testing showed the issue stemmed from many
                    conflicting and corrupted packages presented on each of the
                    affected computers. Resolving the problem involved either
                    removing or reinstalling the problematic packages, which
                    successfully restored the upgrade functionality.
                  </p>
                  <p>
                    Afterward, type command
                    <span class="command">sudo dpkg --configure -a </span> to
                    fix package installation issue
                  </p>
                </div>
              </section>
              <section>
                <h4 id="problem4.1.3">
                  Problem 4.1.3 - Obsolete PostgreSQL version 9.5
                </h4>
                <div class="datetime"><time>22-July-2025</time></div>
                <div class="content">
                  <p>
                    During the upgrade process of all seven computers, a prompt
                    displaying the message "Obsolete major version 9.5"
                    appeared, which halted the upgrade.
                  </p>
                  <p>
                    After consulting with staff and the supervisor, we
                    determined that the associated package is no longer in use.
                    Therefore, we opted to uninstall the package rather than
                    invest time in resolving potential conflicts or upgrading
                    its version. To find and list all the PostgreSQL packages,
                    we ran:
                    <span class="command">dpkg -l | grep postgresql</span>
                  </p>
                  <p>
                    Following the uninstallation of the PostgreSQL package, the
                    OS upgrade processes were resumed.
                  </p>
                </div>
              </section>
              <section>
                <h4 id="problem4.1.4">
                  Problem 4.1.4 - No development version of an LTS available
                </h4>
                <div class="datetime"><time>22-July-2025</time></div>
                <div class="content">
                  <p>
                    During the OS upgrade from version 18.04 to 20.04 on the
                    computers "slave2", "slave3", and "slave4", all three
                    machines failed to execute the command command
                    <span class="inline">sudo do-release-upgrade</span>. Each
                    reported the same error.
                  </p>
                  <pre><code class="language-bash">
"Checking for a new Ubuntu release
Failed to connect to https://changelogs.ubuntu.com/meta-release-lts-development.
Check your Internet connection or proxy settings
There is no development version of an LTS available."
                  </code></pre>
                  <p>
                    After extensive research, we discovered that the URLs used
                    by the update manager to access the repository of available
                    Ubuntu releases were no longer functioning.
                  </p>
                  <p>
                    To resolve this, we manually modified the URLs by running:
                    <span class="command"
                      >sudo nano /etc/update-manager/meta-release</span
                    >
                    and changing <span class="inline">https</span> to
                    <span class="inline">http</span>.
                  </p>
                  <pre><b>/etc/update-manager/meta-release</b><code class="language-bash">
[METARELEASE]
URI = http://changelogs.ubuntu.com/meta-release
URI_LTS = http://changelogs.ubuntu.com/meta-release-lts
                  </code></pre>
                </div>
              </section>
            </div>
          </section>
          <section>
            <h3 id="problem4.2">
              Second vulnerability - SSH Terrapin Prefix Truncation Weakness -
              High Severity
            </h3>
            <div class="content">
              <section>
                <h4 id="problem4.2.1">Problem 4.2.1 - First scan</h4>
                <div class="content">
                  <p>
                    Regarding the second high-severity vulnerability, it was
                    identified that the cryptographic algorithms currently
                    utilized by our SSH server were considered insecure and
                    potentially susceptible to exploitation. Specifically, the
                    following six algorithms were flagged:
                  </p>
                  <ul>
                    <li>chacha20-poly1305@openssh.com</li>
                    <li>umac-64-etm@openssh.com</li>
                    <li>umac-128-etm@openssh.com</li>
                    <li>hmac-sha2-256-etm@openssh.com</li>
                    <li>hmac-sha2-512-etm@openssh.com</li>
                    <li>hmac-sha1-etm@openssh.com</li>
                  </ul>
                  <p>
                    To mitigate this vulnerability, it is recommended to either
                    disabled or replaced these algorithms with more secure
                    alternatives. Disabling them should be sufficient to resolve
                    the issue.
                  </p>
                  <p>
                    To do that, edit the
                    <span class="inline">sshd_config</span> file by command
                    <span class="command">
                      sudo nano /etc/ssh/sshd_config
                    </span>
                    and append the following code to the end of file
                  </p>
                  <pre><b>/etc/ssh/sshd_config</b><code class="language-bash">
# Disable Ciphers List
Ciphers -chacha20-poly1305@openssh.com

#Enable MACs List
MACs hmac-sha1,hmac-sha1-96,hmac-sha2-256,hmac-sha2-512,hmac-md5,hmac-md5-96,umac-64@openssh.com,umac-128@openssh.com,hmac-sha1-96-etm@openssh.com,hmac-md5-etm@openssh.com,hmac-md5-96-etm@openssh.com
                  </code></pre>
                  <p>
                    For unknown reason, multiple exclusions was not working.
                    Only the first one in the list was disabled. User can still
                    connect via other listed MACs algorithm. Therefore, we used
                    explicit inclusion list and avoided using exclusion list for
                    MACs.
                  </p>
                  <pre><b>/etc/ssh/sshd_config</b><code class="language-bash">
# Disable MACs List
MACs -umac-64-etm@openssh.com,-umac-128-etm@openssh.com,-hmac-sha2-256-etm@openssh.com,-hmac-sha2-512-etm@openssh.com,-hmac-sha1-etm@openssh.com
                </code></pre>
                  <p>
                    To list all available Ciphers and MACs supported by OpenSSH,
                    use
                    <span class="inline">ssh -Q mac</span> and
                    <span class="inline">ssh -Q cipher</span>
                  </p>
                  <p>
                    To verify the whether the cryptographic algorithms were
                    disabled, specify the algorithms used in ssh connection. For
                    example:
                    <span class="command"
                      >ssh -c chacha20-poly1305@openssh.com user@ip</span
                    >
                    <span class="command"
                      >ssh -m umac-64-etm@openssh.com user@ip</span
                    >
                  </p>
                  <p>
                    Implementing these modifications is expected to resolve the
                    second high-severity vulnerability.
                  </p>
                </div>
              </section>
              <section>
                <h4 id="problem4.2.2">Problem 4.2.2 - Second scan</h4>
                <div class="content">
                  <p>
                    After the correction of the MACs list, a second security
                    scan was performed and reported by OCIO. The Tenable
                    security scan reported additional insecure MAC algorithms in
                    used, namely:
                  </p>
                  <ul>
                    <li>hmac-md5</li>
                    <li>hmac-md5-96</li>
                    <li>hmac-md5-96-etm@openssh.com</li>
                    <li>hmac-md5-etm@openssh.com</li>
                    <li>hmac-sha1-96</li>
                    <li>hmac-sha1-96-etm@openssh.com</li>
                  </ul>
                  <p>
                    In response, a list of permitted MAC algorithms in
                    <span class="inline">sshd_config</span> was further
                    restricted to enhance security.
                  </p>
                  <pre><b>/etc/ssh/sshd_config</b><code class="language-bash">
#Enable MACs List
MACs hmac-sha1,hmac-sha2-256,hmac-sha2-512,umac-64@openssh.com,umac-128@openssh.com
                  </code></pre>
                </div>
              </section>
            </div>
          </section>
          <section>
            <h3 id="problem4.3">
              Third vulnerability - ICMP Timestamp Request Remote Date
              Disclosure - Low Severity
            </h3>
            <div class="content">
              <p>
                For the third vulnerability, the server maintenance technician
                should Block ICMP Timestamp Requests at the Firewall. Require
                someone familiar with Linus operating systems to access and
                modify the firewall setting.
              </p>
              <p>
                The issue was automatically resolved upon completion of the OS
                upgrade.
              </p>
            </div>
          </section>
        </div>
      </section>
      <section>
        <h2 id="problem5">Problem 5: SSH Connection failure</h2>
        <div class="datetime"><time>11-Jul-2025</time></div>
        <div class="content">
          <p>
            While upgrading the Operating system of the server, there was a
            period during which the SSH service became inaccessible. A thorough
            and extensive investigation revealed that, in <i>Bill01</i>, two
            wired network connections were conflicting, resulting in a mismatch
            between the static public IP address provided by OCIO and the
            dynamic public IP address assigned by our router.
          </p>
          <p>
            On every reboot of <i>Bill01</i>, the dynamic public IP address
            override the static public IP. An network reset and override must be
            performed manually. It hints a potential problem that might recur in
            the future. For reference, the steps we took were logged below.
          </p>
          <section>
            <h3 id="12.1">Mismatch between public IPs</h3>
            <div class="content">
              <p>
                When runing <span class="inline">hostname -I</span> and
                <span class="inline">curl ifconfig.me</span>, different public
                IPs were returned. Our server although connected to the static
                IP network, was not using it as its public IP. Hence, connection
                via static IP, <span class="inline">ssh 175.159.131.248</span>,
                was not working.
              </p>
              <p>
                One may temporarily forward port 22 on the router via its
                setting page at <span class="inline">192.168.1.1</span> with the
                password <span class="inline">billyeung</span>. And enable
                listening on incoming connection from all IPs by editing the
                <span class="inline">sshd_config</span> file:
                <span class="command">sudo nano /etc/ssh/sshd_config</span>
              </p>
              <pre><b>/etc/ssh/sshd_config</b><code class="language-bash">
ListenAddress ::
ListenAddress 0.0.0.0
                </code></pre>
              <p>
                Then, use the dynamic public IP assigned by the router to ssh
                connect the server. As of 10-7-2025, SSH connection was
                temporarily reestablished using the dynamic IP,
                <span class="inline">ssh 175.159.139.164</span>.
              </p>
              <p>
                If the ssh service is still unavailable, one may check and
                restart its status using
                <span class="command">sudo systemctl status ssh</span>
                <span class="command">sudo systemctl restart ssh</span>
              </p>
              <p>
                Check also the firewall to see if connection via port 22 was
                allowed. Allow ssh in the firewall if needed.
                <span class="command">sudo ufw status</span>
                <span class="command">sudo ufw allow ssh</span>
              </p>
              <p>
                This way, one may continue working on the server remotely via
                the dynamic ip address.
              </p>
            </div>
          </section>

          <section>
            <h3 id="12.2">Cause and Replication steps</h3>
            <div class="content">
              <figure>
                <img
                  width="550"
                  src="image\cluster\network_setting.webp"
                  alt=""
                />
                <figcaption>
                  (Top) Ethernet <i>enp0s25</i> refers to the static IP network.
                  (Bottom) Ethernet <i>ens1</i> refers to the dynamic IP
                  network.
                </figcaption>
              </figure>
              <p>
                Extensive investigation revealed that the computer was connected
                to two different networks. The Ethernet interface
                <i>enp0s25</i> connected the computer <i>bill01</i> to the
                external internet, while the Ethernet interface
                <i>ens1</i> linked <i>bill01</i> to the internal intranet,
                allowing its communication and access with the other six
                computers.
              </p>
              <p>
                The order in which the network interfaces were connected
                affected the computer’s public IP address. When the network
                <i>ens1</i> was connected first, bill01 use the dynamic public
                IP address assigned by the router as its public IP address. When
                the network <i>enp0s25</i> was connected first, bill01 use the
                static public IP address assigned by OCIO as its public IP
                address.
              </p>
              <p>
                <b>
                  Therefore, we should always click the <i>enp0s25</i> button
                  first, then the <i>ens1</i> button secondly.
                </b>
                It was confirmed that the static IP address, the SSH service,
                and the firewall were all functioning correctly. The issue
                stemmed solely from the sequence of network connections, which
                altered the public IP address and led to SSH connection
                failures.
              </p>
            </div>
          </section>
          <section>
            <h3 id="12.3">Static IP address</h3>
            <div class="content">
              <figure>
                <img width="450" src="image/cluster/static-ip.webp" alt="" />
                <figcaption>
                  The static public IP, netmask, gateway and DNS provided by
                  OCIO in April 2016.
                </figcaption>
              </figure>
              <p>
                Make sure the network interface <i>enp0s25</i> was setup
                correcly using the static IP address provided by OCIO.
              </p>
            </div>
          </section>
        </div>
      </section>
      <section>
        <h2 id="problem6">
          Problem 6: <span class="inline">ssh hostname</span>: Hostname
          unreachable
        </h2>
        <div class="datetime"><time>11-Jul-2025</time></div>
        <div class="content">
          <p>
            After OS upgrade or system reboot, the local IP address assigned to
            the computers by our router might changed. Therefore, SSH connection
            between computers like
            <span class="inline">ssh master</span> fails. The system tried to
            refer and connect to the old local IP which no longer exist.
          </p>
          <section>
            <h3>Option 1</h3>
            <div class="content">
              <p>
                Check the latest local IP on each computer using either command
                <span class="inline">ip a</span>,
                <span class="inline">ifconfig</span> or
                <span class="inline">hostname -I</span>.
              </p>
            </div>
          </section>
          <section>
            <h3>Option 2</h3>
            <div class="content">
              <p>
                You may also connect to the router webpage via
                <span class="inline">192.168.1.1</span>, login with the password
                <sudo class="inline">billyeung</sudo> and check the list of
                local IP address.
              </p>
            </div>
          </section>
          <section>
            <h3>Option 3</h3>
            <div class="content">
              <p>
                Use <span class="inline">nmap</span> modules to view all the
                devices in the local network. Install the module if needed:
                <span class="command">sudo apt install nmap</span>
              </p>
              <p>
                Then, run the following command to scan the local network. Try
                to locate any new IP and connect to it to check its hostname.
                <span class="command">
                  nmap -sn $(hostname -I | awk '{print $1}')/24
                </span>
              </p>
            </div>
          </section>
          <section>
            <h3>Option 4</h3>
            <div class="content">
              <p>
                Reassign static local IPs to each of the seven compupters. To do
                that, login our router webpage via
                <span class="inline">192.168.1.1</span> with the password
                <span class="inline">billyeung</span>. Assign each of the seven
                computers with a custom static local IP address.
              </p>
              <figure>
                <img width="450" src="image\cluster\local-ip.webp" alt="" />
                <figcaption>
                  On 17-July-2025, new custom static local IP addresses were
                  assigned to each of the seven computers.
                </figcaption>
              </figure>
            </div>
          </section>
          <section>
            <h3>Solution</h3>
            <div class="content">
              <p>
                Finally, update the file
                <span class="inline">/etc/hosts</span> with the latest local IP
                address corresponding to each computers hostname. Repeat and
                synchronize the file across all seven computers.
              </p>
              <p>
                The file in <span class="inline">/etc/hosts</span> stores the
                hostname and their corresponding IP address.
              </p>
              <ol>
                <li>The files should be the same across all computers.</li>
                <li>
                  And the IP should match the assigned local IP by the router.
                </li>
              </ol>
              <p>
                If not, use <span class="inline">sudo nano /etc/hosts</span> to
                edit the file.
              </p>
              <p>As of 30/6/2023, this file in computer <i>master</i>:</p>
              <pre><b>/etc/hosts</b><code class="language-bash">
#########################################################
#127.0.0.1 localhost
#127.0.1.1 bill03
192.168.1.143 master
192.168.1.104 bill01
192.168.1.102 slave1
192.168.1.224 slave2
192.168.1.12 slave3
192.168.1.100 slave4
192.168.1.103 slave5
#175.159.131.248 bill-workstation
#202.45.51.232 master
#202.45.51.231 slave1
# the following lines are desirable for ipv6 capable hosts
::1 ip6-localhost ip6-loopback
fe00::0 ip6-localnet
ff00::0 ip6-mcastprefix
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
#########################################################
                </code>
              </pre>
              <p>
                As of July 17, 2025, each of the seven computers was assigned a
                new static local IP address. Their hosts files have been updated
                to incorporate these changes accordingly.
              </p>
              <pre><b>/etc/hosts</b><code class="language-bash">
#########################################################
127.0.0.1       localhost
127.0.1.1       this_computer_hostname

# The following lines are desirable for IPv6 capable hosts
::1     ip6-localhost ip6-loopback
fe00::0 ip6-localnet
ff00::0 ip6-mcastprefix
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
192.168.1.100 Bill01-master
192.168.1.101 slave1
192.168.1.102 slave2
192.168.1.103 slave3
192.168.1.104 slave4
192.168.1.105 slave5
192.168.1.106 slave6
#########################################################
              </code>
            </pre>
            </div>
          </section>
        </div>
      </section>
      <section>
        <h2 id="problem7">Bill01: Locked Display Resolution at 640x480</h2>
        <div class="datetime"><time>23-Jul-2025</time></div>
        <div class="content">
          <p>
            Due to a previous attempt to resolve issues with executing
            <span class="inline">do-release-upgrades</span>, the update manager
            was configured to allow upgrades to non-LTS versions of Ubuntu.
          </p>
          <p>
            Specifically, the setting
            <span class="inline">prompt=lts</span> was changed to
            <span class="inline">prompt=normal</span> in the configuration file
            using the command:
            <span class="command"
              >sudo nano /etc/update-manager/release-upgrades</span
            >
          </p>
          <p>
            This change was not reverted, resulting in the system upgrading to
            Ubuntu 25.04. Since this upgrade, the display on <em>Bill01</em> has
            been locked at a resolution of 640x480.
          </p>
          <p>
            Extensive investigation revealed that the last officially supported
            driver for Bill01's Quadro K420 graphics card is Nvidia 470, which
            is not available in the Ubuntu 25.04 repositories. Ubuntu 25.04
            includes only Nvidia drivers versions 535 to 575, which do not
            support the Quadro K420.
          </p>
          <p>
            Consequently, command like
            <span class="inline">nvidia-smi</span> failed to communicate with
            the driver, indicating that the Nvidia 470 driver was not loaded or
            active.
          </p>
          <p>
            To resolve this issue, we first purged all Nvidia drivers installed
            on the system with:
            <span class="command">sudo apt purge '^nvidia-.*'</span>
          </p>
          <p>
            Then, we installed the open-source Nouveau graphics driver with:
            <span class="command"
              >sudo apt install xserver-xorg-video-nouveau</span
            >
          </p>
          <p>
            After rebooting the system, the display resolution options were
            restored, allowing for higher display resolutions to be used.
          </p>
        </div>
      </section>
      <section>
        <h2 id="direct_reinstall">
          Final Decision: Direct Reinstallation of Ubuntu 24.04 LTS
        </h2>
        <div class="datetime"><time>23-Jul-2025</time></div>
        <div class="content">
          <p>
            Despite significant efforts to upgrade the operating systems and
            troubleshoot the seven computers through versions 16.04, 18.04,
            20.04, 22.04, and 24.04, the system files remained heavily
            corrupted, leading to persistent issues.
          </p>
          <p>Specifically</p>
          <ul>
            <li>
              <b>Slave1</b>: The GUI was inaccessible. Further investigation
              revealed that the system was in a mixed state of versions 20.04
              and 24.04. As a result, the internal updater continuously
              requested packages for 24.04 while the system was still running
              20.04, causing errors.
            </li>
            <li>
              <b>Slave2</b>: The GUI was non-functional. The taskbar, menu, and
              "Open Terminal" option via right-click were missing, leaving only
              the background image visible.
            </li>
            <li>
              <b>Slave5</b>: The GUI also appeared broken, with the screen
              turning off and on repeatedly every few seconds.
            </li>
          </ul>
          <p>
            Given these problems, the final decision was to erase the disks of
            all computers expect <em>bill01</em>. And perform a clean
            reinstallation of Ubuntu version 24.04. This approach ensures that
            all previous data and system problems were removed, resulting in
            fully operational and clean systems.
          </p>
          <p>
            To facilitate synchronization and streamline the setup process
            across master and slave1-5 computers, four additional shell scripts,
            namely
            <span class="inline">backupData.sh</span>,
            <span class="inline">ssh_setup.sh</span>,
            <span class="inline">desktop_setup.sh</span> and
            <span class="inline">hostname_setup.sh</span> were written and to be
            executed on all computers.
          </p>
          <pre><b id="backupData">backupData.sh</b><code class="language-bash">
#!/bin/bash

[ -z "$1" ] && echo "Please provide a folder name." && exit 1
#DEST reter to USB destination
DEST="/mnt/usb/$1"
#DEST="/media/bill/PHY LAB 04/Backup/$1"
echo "backing up to $DEST"
[ ! -d "$DEST" ] && sudo mkdir -p "$DEST"
sudo cp /etc/passwd /etc/shadow /etc/group /etc/gshadow /etc/sudoers "$DEST"
[ -d "/etc/sudoers.d" ] && sudo cp -r /etc/sudoers.d "$DEST"

          </code></pre>
          <pre><b id="ssh_setup">ssh_setup.sh</b><code class="language-bash">
#!/bin/bash

# Exit if any command fails
set -e
# Ensure the script is run as root
if [ "$EUID" -ne 0 ]; then
  echo "❌ Please run as root or with sudo"
  exit 1
fi

echo "Set power mode to performance" 
powerprofilesctl set performance
echo "✅ Power mode set to Performance."
	
echo "Installing OpenSSH server..."
apt update
apt install -y openssh-server
echo "✅ OpenSSH installed"

echo "Starting SSH service..."
systemctl start ssh
echo "✅ OpenSSH started"

echo "Enabling SSH service to start on boot..."
systemctl enable ssh
echo "✅ OpenSSH enabled on startup"

echo "Setting up UFW (Uncomplicated Firewall)..."
ufw allow ssh
ufw --force enable
echo "✅ Firewall enabled"

ssh_folder="/home/$SUDO_USER/.ssh"
echo "Removing previous SSH host keys in $ssh_folder"
if [ -f "$ssh_folder/known_hosts" ]; then
rm -f "$ssh_folder/known_hosts"
echo "✅ $ssh_folder/known_hosts file cleared"
else
echo "✅ No Known_hosts file"
fi
if [ -f "$ssh_folder/known_hosts.old" ]; then
rm -f "$ssh_folder/known_hosts.old"
echo "✅ $ssh_folder/known_hosts.old file cleared"
else
echo "✅ No Known_hosts.old file"
fi
echo "✅ SSH and firewall setup complete. 🎉"

echo "Remove insecure Cryptographic Algorithms"
CONFIG_FILE="/etc/ssh/sshd_config"
CIPHERS_LINE="Ciphers -chacha20-poly1305@openssh.com"
MACS_LINE="MACs hmac-sha1,hmac-sha2-256,hmac-sha2-512,umac-64@openssh.com,umac-128@openssh.com"

# Function to check and append if necessary
append_if_missing() {
    local line="$1"
    local comment="$2"
    if ! grep -Fxq "$line" "$CONFIG_FILE"; then
        echo "" >> "$CONFIG_FILE"
        echo "# $comment" >> "$CONFIG_FILE"
        echo "$line" >> "$CONFIG_FILE"
        echo "✅ Appended in $CONFIG_FILE:"
        echo "$line"
    else
        echo "✅ Already exists in $CONFIG_FILE:"
        echo "$line"
    fi
}

# Run the checks
append_if_missing "$CIPHERS_LINE" "Disable Ciphers List"
append_if_missing "$MACS_LINE" "Enable MACs List"
          </code></pre>
          <pre><b id="desktop_setup">desktop_setup.sh</b><code class="language-bash">
#!/bin/bash

# Exit if any command fails
set -e
# 🛑 Abort if running as root
if [ "$EUID" -eq 0 ]; then
  echo "❌ Don't run this script as root. Run it as a regular user."
  exit 1
fi

if ! command -v dbus-launch &> /dev/null
then
    echo "✅ dbus-launch not found. Installing dbus-x11..."
    sudo apt update
    sudo apt install -y dbus-x11
else
    echo "✅ dbus-launch is already installed."
fi

echo "Mimicing Windows Desktop Style"
gsettings set org.gnome.shell.extensions.dash-to-dock dock-position 'BOTTOM'
echo "✅ Dock moved to the bottom."
gsettings set org.gnome.shell.extensions.dash-to-dock show-apps-at-top true
gsettings set org.gnome.shell.extensions.dash-to-dock show-apps-always-in-the-edge true
echo "✅ App button moved to the left."
gsettings set org.gnome.shell.extensions.ding start-corner 'top-left'
gsettings set org.gnome.shell.extensions.ding arrangeorder 'NAME'
gsettings set org.gnome.shell.extensions.ding keep-arranged true
echo "✅ Desktop icon arranged to the top left."
gsettings set org.gnome.shell favorite-apps "['org.gnome.Nautilus.desktop', 'org.gnome.Terminal.desktop', 'firefox_firefox.desktop']"
echo "✅ Pinned Folder, Terminal, Browswer to the taskbar."

echo "Disable screen blanking on idle"
gsettings set org.gnome.desktop.session idle-delay 0
echo "✅ Screen blanking disabled."
          </code></pre>
          <pre><b id="hostname_setup">hostname_setup.sh</b><code class="language-bash">
#!/bin/bash
# Exit if any command fails
set -e
# Ensure the script is run as root
if [ "$EUID" -ne 0 ]; then
  echo "❌ Please run as root or with sudo"
  exit 1
fi

# List of IP-hostname pairs
HOST_ENTRIES=(
  "192.168.1.100 Bill01-master"
  "192.168.1.101 slave1"
  "192.168.1.102 slave2"
  "192.168.1.103 slave3"
  "192.168.1.104 slave4"
  "192.168.1.105 slave5"
  "192.168.1.106 slave6"
)

echo "🔍 Checking /etc/hosts for existing entries..."

for entry in "${HOST_ENTRIES[@]}"; do
  if ! grep -Fqx "$entry" /etc/hosts; then
    echo "$entry" >> /etc/hosts
    echo "➕ Added: $entry"
  else
    echo "✅ Already exists: $entry"
  fi
done

echo "🚀 Host entries update complete!"
          </code></pre>
        </div>
      </section>
      <section>
        <h2 id="NFS">Setting up Network File System (NFS)</h2>
        <div class="content">
          <p>
            To allow job exeuting on <b>Slave</b> computers able to read and
            write files on the <b>Master</b> computer, we need to setup NFS on
            the <b>Master</b> computer and mount the NFS share on each of the
            <b>Slave</b> computers.
          </p>
          <p>
            This way, script distributed to and running on any of the
            <b>Slave</b> computers can read stuff like libraries, data, python
            modules stored in <b>Master</b> computer. And it can also returns
            result and write back to <b>Master</b> computer.
          </p>
          <p>
            To set up NFS, we need to install the server package on
            <b>Bill01-master</b> computer and client package on
            <b>Slave</b> computers.
          </p>
          <section>
            <h3 id="NFS_server">NFS-server: Bill01-master</h3>
            <div class="datetime"><time>22-Auguest-2025</time></div>
            <div class="content">
              <p>
                First, we install the NFS server package in
                <b>Bill01-master</b> computer:
                <span class="command">sudo apt install nfs-kernel-server</span>
              </p>
              <p>
                Enable auto-start of the NFS server on every boot by:
                <span class="command"
                  >sudo systemctl enable nfs-kernel-server</span
                >
              </p>
              <p>
                Start the NFS server right now by:
                <span class="command"
                  >sudo systemctl start nfs-kernel-server</span
                >
              </p>
              <p>
                Check if NFS server is running:
                <span class="command"
                  >sudo systemctl status nfs-kernel-server</span
                >
              </p>
              <p>
                One of the important step offen omitted by internet guide is to
                allow port 2049 in the firewall. Otherwise,
                <b>Slave</b> computers cannot connect to the NFS server although
                the server was running.
              </p>
              <p>
                Allow port 2049 in the firewall by:
                <span class="command">sudo ufw allow 2049</span>
              </p>
              <p>
                Check for change by:
                <span class="command">sudo ufw status</span>
              </p>
              <p>
                Enter in <span class="inline">/etc/exports</span> by:
                <span class="command">sudo nano /etc/exports</span>
              </p>
              <pre><b>/etc/exports</b><code class="language-bash">
/home *(rw,sync,no_subtree_check)
              </code></pre>
              <p>
                This allows <span class="inline">*</span> all hosts to connect
                to <span class="inline">/home</span> in the server
              </p>
              <p>
                Restart the NFS server to load the changes:
                <span class="command"
                  >sudo systemctl restart nfs-kernel-server</span
                >
              </p>
              <p>
                Check if the NFS server is running flawlessly again:
                <span class="command"
                  >sudo systemctl status nfs-kernel-server</span
                >
              </p>
            </div>
          </section>
          <section>
            <h3 id="NFS_client">NFS-client: Slave1-6</h3>
            <div class="content">
              <p>
                Second, we install the NFS client packages on
                <b>Slave</b> computers.
                <span class="command">sudo apt install nfs-common</span>
              </p>
              <p>
                NFS client packages is a set of tool for accessing the NFS
                server. Therefore, we do not need to enable or start it.
              </p>
              <section>
                <h4>Manual mount</h4>
                <div class="content">
                  <p>
                    Now, we test mount the /home directory from
                    <b>Bill01-master</b> computer to <b>Slave</b> computers:
                  </p>
                  <p>
                    We first create a mount point, a folder holding the NFS
                    server directory, in
                    <span class="inline">/mnt/Bill01-master/home</span>:
                    <span class="command"
                      >sudo mkdir -p /mnt/Bill01-master/home</span
                    >
                  </p>
                  <p>
                    Then, make sure the local-ip address of
                    <b>Bill01-master</b> is
                    <span class="inline">192.168.1.100</span> and run:
                    <span class="command"
                      >sudo mount 192.168.1.100:/home /mnt/Bill01-master/home
                    </span>
                  </p>
                  <p>
                    If everything went right, we now will be able to view and
                    modify the
                    <span class="inline">/home</span> directory from
                    <b>Bill01-master</b> in <b>Slave</b> computers
                    <span class="inline">/mnt/Bill01-master/home</span>:
                    <span class="command">ls /mnt/Bill01-master/home</span>
                  </p>
                  <p>
                    Now, let's unmount the NFS server for now:
                    <span class="command"
                      >sudo umount /mnt/Bill01-master/home</span
                    >
                  </p>
                  <p>
                    The disadvantage of using
                    <span class="inline">mount</span> command is that, it does
                    not persist after reboot. The mount will be lost and require
                    manual mount again.
                  </p>
                  <p>
                    To solve that, we employ an auto mount package,
                    <span class="inline">Autofs</span>.
                  </p>
                </div>
              </section>
              <section>
                <h4>Auto mount (Autofs)</h4>
                <div class="content">
                  <p>
                    First, we install the package,
                    <span class="inline">Autofs</span> in
                    <b>Slave</b> computers.
                    <span class="command">sudo apt install autofs</span>
                  </p>
                  <p>
                    Enable and start <span class="line">Autofs</span> to make
                    sure it is up and running. Enable auto-start of the
                    <span class="inline">Autofs</span> on every boot by:
                    <span class="command">sudo systemctl enable autofs</span>
                  </p>
                  <p>
                    Start the <span class="inline">Autofs</span> right now by:
                    <span class="command">sudo systemctl start autofs</span>
                  </p>
                  <p>
                    Check if <span class="inline">Autofs</span> is running:
                    <span class="command">sudo systemctl status autofs</span>
                  </p>
                  <p>
                    Enter and append at the end in
                    <span class="inline">/etc/auto.master</span> by:
                    <span class="command">sudo nano /etc/auto.master</span>
                  </p>
                  <pre><b>/etc/auto.master</b><code class="language-bash">
/mnt/Bill01-master /etc/auto.nfs --timeout=60 --ghost
                  </code></pre>
                  <p>
                    Enter in <span class="inline">/etc/auto.nfs</span> by:
                    <span class="command">sudo nano /etc/auto.nfs</span>
                  </p>
                  <pre><b>/etc/auto.master</b><code class="language-bash">
home -fstype=nfs4,rw 192.168.1.100:/home
                  </code></pre>
                  <p>
                    Restart <span class="inline">Autofs</span> to load the
                    changes:
                    <span class="command">sudo systemctl restart autofs</span>
                  </p>
                  <p>
                    If everything went right, we now will be able to view and
                    modify the
                    <span class="inline">/home</span> directory from
                    <b>Bill01-master</b> in <b>Slave</b> computers
                    <span class="inline">/mnt/Bill01-master/home</span>:
                    <span class="command">ls /mnt/Bill01-master/home</span>
                  </p>
                  <p>
                    The advantage of <span class="inline">Autofs</span> is that
                    it performs on demand mounting and automatically unmounting.
                    The system will only mount the NFS when accessed, and
                    unmount the NFS after a period of inactivity. Hence, freeing
                    system resources and reducing unnecessary mounts.
                  </p>
                </div>
              </section>
            </div>
          </section>
          <section>
            <h3 id="NFS_investigation">Investigation on the old NFS setup</h3>
            <div class="datetime"><time>23-Aug-2025</time></div>
            <div class="content">
              <p>
                In this section, we will be looking at the original method the
                Network File System was setup in the Physics Laboratory Server.
                And discuss its three main flaws.
              </p>
              <section>
                <h4>Problem 1: The use of mount command</h4>
                <div class="datetime"><time>23-Aug-2025</time></div>
                <div class="content">
                  <p>
                    According to the old documentation left from the university,
                    they used
                    <span class="inline">mount</span> command to mount the NFS
                    in the old day.
                    <span class="command">sudo mount bill01:/home ./home</span>
                  </p>
                  <p>
                    However, <span class="inline">mount</span> command does not
                    persist after reboot. The mount will be lost on every reboot
                    and it require manual mounting again.
                  </p>
                </div>
              </section>
              <section>
                <h4>Problem 2: Dynamic local ip in /etc/exports</h4>
                <div class="datetime"><time>23-Aug-2025</time></div>
                <div class="content">
                  <p>
                    The original content of
                    <span class="inline">/etc/exports</span> is shown below:
                  </p>
                  <pre><b>/etc/exports</b><code class="language-bash">
/home 192.168.1.102(rw)
/home 192.168.1.143(rw)
/home 192.168.1.224(rw)
/home 192.168.1.12(rw)
/home 192.168.1.100(rw)
/home 192.168.1.103(rw)
                            </code></pre>
                  <p>
                    It set read and write permission of the NFS folder to the
                    above local-ip addresses. However the local-ip of the
                    <b>Slave</b> computers were not fixed back then.
                    Occasionally their ip changed. So, they were no longer
                    permitted to access the NFS server.
                  </p>
                </div>
              </section>
              <section>
                <h4>Problem 3: Masking of the /home directory</h4>
                <div class="datetime"><time>23-Aug-2025</time></div>
                <div class="content">
                  <p>
                    Looking at the mount command again.
                    <span class="command">sudo mount bill01:/home ./home</span>
                  </p>
                  <p>
                    We can see they used to mount the
                    <span class="inline">/home</span> directory of
                    <b>Bill01</b> to <span class="inline">/home</span> directory
                    of <b>Slaves</b> computers. Therefore, masking the original
                    <span class="inline">/home</span> directory on
                    <b>Slave</b> computers. It also means if we type
                    <span class="inline">cd /home</span> on
                    <b>Slave</b>
                    computers, we would be redirected to and accessing
                    <span class="inline">/home</span> from <b>Bill01</b>. I
                    don't know if this is a good idea or not, right now, I set
                    the mount point to
                    <span class="inline">/mnt/Bill01-master/home</span> instead.
                  </p>
                </div>
              </section>
              <p>
                All in all, account for the fact that the person setting up the
                server before left the university long time ago and the
                computers had been rebooted many times. I think nobody had the
                knowledge to know they need to remount the NFS and reset the
                local ip in
                <span class="inline">/etc/exports</span>. Therefore we can
                safely assume the Network File System was always not working.
              </p>
            </div>
          </section>
        </div>
      </section>
    </main>
    <!-- custom script -->
    <script src="script/script.js"></script>
  </body>
</html>
